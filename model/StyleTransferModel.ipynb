{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Перенос стиля с помощью архитектуры *Adaptive Instance Normalization*</center>\n",
    "Сделаем необходимые импорты и инициализируем константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from net.style_autoencoder import StyleTransfer\n",
    "IMAGES_FOLDER = '/mnt/s/CV/StyleTransferData/'\n",
    "CONTENT_FOLDER = os.path.join(IMAGES_FOLDER, 'test2015')\n",
    "STYLE_FOLDER = os.path.join(IMAGES_FOLDER, 'wikiart')\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "IMG_SIZE = (256, 256)\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для удаления повреждённых файлов (необходимо раскомментировать последние 2 строки ячейки, если появляются проблемы с чтением файлов во время обучения)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_corrupted_imgs(path) -> None:\n",
    "    def remove(file) -> None:\n",
    "        print('=='*10)\n",
    "        os.remove(os.path.join(path, file))\n",
    "        print(f'Deleted corrupted {file}')\n",
    "        \n",
    "    files = os.listdir(path)\n",
    "    for file in files:\n",
    "        try:\n",
    "            img = tf.io.decode_jpeg(tf.io.read_file(os.path.join(path, file)))\n",
    "            if img is None:\n",
    "                remove(file)\n",
    "        except Exception as e:\n",
    "            remove(file)\n",
    "\n",
    "# delete_corrupted_imgs(STYLE_FOLDER)\n",
    "# delete_corrupted_imgs(CONTENT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем 2 датасета: `content_dataset` - для изображений контента и `style_dataset` - для стилей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 81434 files.\n",
      "Found 81434 files.\n"
     ]
    }
   ],
   "source": [
    "dataset_config = dict(\n",
    "    label_mode=None, \n",
    "    labels=None,\n",
    "    shuffle=True,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=None,\n",
    "    crop_to_aspect_ratio=True\n",
    ")\n",
    "\n",
    "content_dataset = keras.utils.image_dataset_from_directory(\n",
    "    CONTENT_FOLDER,\n",
    "    **dataset_config\n",
    ")\n",
    "style_dataset = keras.utils.image_dataset_from_directory(\n",
    "    STYLE_FOLDER,\n",
    "    **dataset_config\n",
    ")\n",
    "\n",
    "dataset = tf.data.Dataset.zip((content_dataset, style_dataset)).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<b>Ссылка на оригинальную статью <i>AdaIN</i></b>](https://arxiv.org/pdf/1703.06868.pdf)\n",
    "\n",
    "[<b>Моя реализация данной модели на <i>Keras / Tensorflow</i></b>](net/style_autoencoder.py)\n",
    "\n",
    "В отличии от оригинального решения, я использовал вес функции потерь стиля $\\lambda = 20$ вместо $\\lambda = 0.01$, чтобы использовать меньше эпох для обучения и получить \"больше стиля\" в результирующем изображении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"style_transfer\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"style_transfer\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ ada_in (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AdaIN</span>)                  │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)               │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,505,728</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)               │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,505,219</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ ada_in (\u001b[38;5;33mAdaIN\u001b[0m)                  │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder (\u001b[38;5;33mEncoder\u001b[0m)               │ ?                      │     \u001b[38;5;34m3,505,728\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder (\u001b[38;5;33mDecoder\u001b[0m)               │ ?                      │     \u001b[38;5;34m3,505,219\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,010,947</span> (26.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,010,947\u001b[0m (26.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,505,219</span> (13.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,505,219\u001b[0m (13.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,505,728</span> (13.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,505,728\u001b[0m (13.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = StyleTransfer()\n",
    "optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=keras.optimizers.schedules.InverseTimeDecay(\n",
    "        initial_learning_rate=1e-4,\n",
    "        decay_steps=10,\n",
    "        decay_rate=5e-5\n",
    "    )\n",
    ")\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "model.compile(optimizer=optimizer, loss_fn=loss_fn)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1710173606.162931  159815 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10179/10179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m972s\u001b[0m 93ms/step - content_loss: 471015.2188 - learning_rate: 9.8755e-05 - style_loss: 1417284.3750 - total_loss: 1888297.5000\n",
      "Epoch 2/5\n",
      "\u001b[1m10179/10179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m954s\u001b[0m 93ms/step - content_loss: 316740.0625 - learning_rate: 9.4029e-05 - style_loss: 539033.7500 - total_loss: 855772.7500\n",
      "Epoch 3/5\n",
      "\u001b[1m10179/10179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m974s\u001b[0m 95ms/step - content_loss: 284312.5000 - learning_rate: 8.9734e-05 - style_loss: 465085.6875 - total_loss: 749398.1250\n",
      "Epoch 4/5\n",
      "\u001b[1m10179/10179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m977s\u001b[0m 96ms/step - content_loss: 268049.6562 - learning_rate: 8.5814e-05 - style_loss: 431344.8438 - total_loss: 699394.4375\n",
      "Epoch 5/5\n",
      "\u001b[1m10179/10179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m984s\u001b[0m 96ms/step - content_loss: 256738.2969 - learning_rate: 8.2223e-05 - style_loss: 407157.5625 - total_loss: 663895.6250\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            './checkpoint/model_20_style_loss_{epoch}.keras',\n",
    "            monitor='total_loss',\n",
    "            save_best_only=False,\n",
    "            save_freq='epoch'\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты обучения будут в [<b>другом ноутбуке</b>](InferenceTest.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
